{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8615f213",
   "metadata": {},
   "source": [
    "# Contenido\n",
    "\n",
    "1. **Carga de librerías y datos**  \n",
    "   - 1.a. Librerías  \n",
    "   - 1.b. Funciones  \n",
    "   - 1.c. Carga de datos  \n",
    "\n",
    "2. **Modelado**  \n",
    "   - 2.a. Creación variables exógenas\n",
    "   - 2.b. Entrenamiento del modelo\n",
    "   - 2.c. Evaluación\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cef18937-67d3-441c-9f41-6c98b09d7cd2",
   "metadata": {},
   "source": [
    "# 1. Carga de librerías y datos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25216ec5-a69b-429a-aff8-40f47d54e9af",
   "metadata": {},
   "source": [
    "##  1.a. Librerías"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf523337-3743-4e93-9cf2-5cf208ada09e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Modelado y Forecasting\n",
    "# ==============================================================================\n",
    "import xgboost\n",
    "import lightgbm\n",
    "import catboost\n",
    "import sklearn\n",
    "from xgboost import XGBRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "from catboost import CatBoostRegressor\n",
    "from sklearn.ensemble import HistGradientBoostingRegressor\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.compose import make_column_transformer\n",
    "from sklearn.compose import make_column_selector\n",
    "\n",
    "import skforecast\n",
    "from skforecast.ForecasterBaseline import ForecasterEquivalentDate\n",
    "from skforecast.ForecasterAutoreg import ForecasterAutoreg\n",
    "from skforecast.model_selection import bayesian_search_forecaster\n",
    "from skforecast.model_selection import backtesting_forecaster\n",
    "from skforecast.model_selection import select_features\n",
    "import shap\n",
    "\n",
    "import cloudpickle\n",
    "import pandas as pd\n",
    "from skforecast.model_selection import backtesting_forecaster\n",
    "import numpy as np\n",
    "import plotly.graph_objects as go"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "525ecedb-5cde-41c8-a5ed-945094628e3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Creacion_exog import calculo_variables_exogenas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de78c47a-3d53-46bc-8de3-0b537f10cdbf",
   "metadata": {},
   "source": [
    "## 1.b. Funciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6bbe1780-cbde-4229-9f21-8f2c2c01bf6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_val_test(datos, fin_train, fin_val):\n",
    "    # Separación de datos en entrenamiento, validación y test\n",
    "    # ==============================================================================\n",
    "    datos_train = datos.loc[: fin_train]\n",
    "    datos_val   = datos.loc[fin_train:fin_val]\n",
    "    datos_test  = datos.loc[fin_val:]\n",
    "    print(\n",
    "        f\"Fechas train      : {datos_train.index.min()} --- {datos_train.index.max()}  \"\n",
    "        f\"(n={len(datos_train)})\"\n",
    "    )\n",
    "    print(\n",
    "        f\"Fechas validación : {datos_val.index.min()} --- {datos_val.index.max()}  \"\n",
    "        f\"(n={len(datos_val)})\"\n",
    "    )\n",
    "    print(\n",
    "        f\"Fechas test       : {datos_test.index.min()} --- {datos_test.index.max()}  \"\n",
    "        f\"(n={len(datos_test)})\"\n",
    "    )\n",
    "    return datos_train, datos_val, datos_test\n",
    "\n",
    "def imputar_nulos_por_hora(datos):\n",
    "    datos.index = pd.to_datetime(datos.index)\n",
    "    horas = datos.index.hour\n",
    "    \n",
    "    # Promedio por hora y sustitución el valores Nan\n",
    "    media_por_hora = datos.groupby(horas).transform('mean')\n",
    "    datos = datos.fillna(media_por_hora)\n",
    "    \n",
    "    return datos\n",
    "\n",
    "def auxiliar(variables_exogenas):\n",
    "    # Selección de variables exógenas a incluir en el modelo\n",
    "    # ==============================================================================\n",
    "    exog_cols = []\n",
    "    # Columnas que terminan con _seno o _coseno son seleccionadas\n",
    "    exog_cols.extend(variables_exogenas.filter(regex='_seno$|_coseno$').columns.tolist())\n",
    "    \n",
    "    # Columnas que empiezan con festivo_ son seleccionadas\n",
    "    exog_cols.extend(variables_exogenas.filter(regex='^festivo_.*').columns.tolist())\n",
    "    exog_cols.extend(['FESTIVO'])\n",
    "    \n",
    "    variables_exogenas = variables_exogenas.filter(exog_cols, axis=1)\n",
    "    return exog_cols, variables_exogenas\n",
    "\n",
    "def tratamiento(datos, variables_exogenas, fin_train, fin_val):\n",
    "    # Combinar variables exógenas y target en el mismo dataframe\n",
    "    # ==============================================================================\n",
    "    datos = datos[['CANTIDAD']].merge(\n",
    "        variables_exogenas,\n",
    "        left_index=True,\n",
    "        right_index=True,\n",
    "        how='left'\n",
    "    )\n",
    "    \n",
    "    # Debido a la creación de medias móviles, hay valores NaN al principio de la serie.\n",
    "    # Debido a holiday_dia_siguiente hay valores NaN al final de la serie.\n",
    "    # Las columnas numéricas se convierten a float32.\n",
    "    datos = datos.dropna()\n",
    "    datos = datos.astype({col: np.float32 for col in datos.select_dtypes(\"number\").columns})\n",
    "    datos_train = datos.loc[: fin_train, :].copy()\n",
    "    datos_val   = datos.loc[fin_train:fin_val, :].copy()\n",
    "    datos_test  = datos.loc[fin_val:, :].copy()\n",
    "    return datos, datos_train, datos_val, datos_test\n",
    "\n",
    "def prepare_time_series(data, column_name='CANTIDAD', freq='H'):\n",
    "    data = data.to_frame(name=column_name)\n",
    "    data.index = pd.to_datetime(data.index)\n",
    "    data = data.asfreq(freq)\n",
    "    data.index.name = 'FECHA'\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28923c97-01c8-420a-8544-773606c5bd58",
   "metadata": {},
   "source": [
    "## 1.c. Carga de datos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1fd32e5d-a0bf-4448-ae74-5cdf8ccc6be3",
   "metadata": {},
   "outputs": [],
   "source": [
    "info = pd.read_csv('../../1-DATOS/3-DATOS DE RESULTADOS/ANALISIS DESCRIPTIVO/info_clusters.csv', index_col = 0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "94851a75-1463-490c-a04a-dba0be8a2f58",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet('../../1-DATOS/2-DATOS PROCESADOS/BICING/INFORMACION COMPLETA/BICICLETAS_HORARIO_2022_2023_FILTRADO.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7af8deb-157d-46d2-bf8f-8ef2ccc88248",
   "metadata": {},
   "source": [
    "# 2. Modelado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "488e5b66-6e8f-4b91-8a0f-c3f00c7fb1bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformación: codificación ordinal + conversión a tipo \"category\"\n",
    "# ==============================================================================\n",
    "pipeline_categorical = make_pipeline(\n",
    "    OrdinalEncoder(\n",
    "        dtype=int,\n",
    "        handle_unknown=\"use_encoded_value\",\n",
    "        unknown_value=-1,\n",
    "        encoded_missing_value=-1\n",
    "    ),\n",
    "    FunctionTransformer(\n",
    "        func=lambda x: x.astype('category'),\n",
    "        feature_names_out= 'one-to-one'\n",
    "    )\n",
    ")\n",
    "\n",
    "transformer_exog = make_column_transformer(\n",
    "    (\n",
    "        pipeline_categorical,\n",
    "        make_column_selector(dtype_include=['category', 'object']),\n",
    "    ),\n",
    "    remainder=\"passthrough\",\n",
    "    verbose_feature_names_out=False,\n",
    ").set_output(transform=\"pandas\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2c39728a-54ec-4fbf-a552-730827b76f7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Espacio de búsqueda de hiperparámetros\n",
    "def search_space(trial):\n",
    "    search_space  = {\n",
    "        'n_estimators'    : trial.suggest_int('n_estimators', 400, 1200, step=100),\n",
    "        'max_depth'       : trial.suggest_int('max_depth', 3, 10, step=1),\n",
    "        'min_data_in_leaf': trial.suggest_int('min_data_in_leaf', 25, 500),\n",
    "        'learning_rate'   : trial.suggest_float('learning_rate', 0.01, 0.5),\n",
    "        'feature_fraction': trial.suggest_float('feature_fraction', 0.5, 1, step=0.1),\n",
    "        'max_bin'         : trial.suggest_int('max_bin', 50, 250, step=25),\n",
    "        'reg_alpha'       : trial.suggest_float('reg_alpha', 0, 1, step=0.1),\n",
    "        'reg_lambda'      : trial.suggest_float('reg_lambda', 0, 1, step=0.1)\n",
    "    } \n",
    "    return search_space"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "380a6301",
   "metadata": {},
   "source": [
    "Los resultados mostrados en la siguiente celda corresponden a un ejemplo reducido, donde únicamente se han generado 3 modelos (ya que la ejecución completa implica un elevado coste computacional). Sin embargo, el código proporcionado reproduce la creación de todos los modelos predictivos utilizados en el proyecto. Para la realización del mismo, se ejecutó el código completo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b247cb3-e462-4d4c-bb95-ee949727c4eb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Procesando columna: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 19. Best value: 4.02418: 100%|██████████| 20/20 [06:55<00:00, 20.77s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "`Forecaster` refitted using the best-found lags and parameters, and the whole data set: \n",
      "  Lags: [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48] \n",
      "  Parameters: {'n_estimators': 600, 'max_depth': 7, 'min_data_in_leaf': 466, 'learning_rate': 0.011027759675487456, 'feature_fraction': 0.9, 'max_bin': 150, 'reg_alpha': 0.9, 'reg_lambda': 0.5}\n",
      "  Backtesting metric: 4.024178095800369\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 273/273 [00:09<00:00, 28.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El tiempo empleado para la columna 1 ha sido: 436.52 segundos\n",
      "Procesando columna: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 19. Best value: 3.35293: 100%|██████████| 20/20 [07:17<00:00, 21.88s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "`Forecaster` refitted using the best-found lags and parameters, and the whole data set: \n",
      "  Lags: [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48] \n",
      "  Parameters: {'n_estimators': 600, 'max_depth': 7, 'min_data_in_leaf': 493, 'learning_rate': 0.010549277719587623, 'feature_fraction': 0.5, 'max_bin': 125, 'reg_alpha': 0.2, 'reg_lambda': 0.5}\n",
      "  Backtesting metric: 3.352933440242877\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 273/273 [00:10<00:00, 26.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El tiempo empleado para la columna 2 ha sido: 456.94 segundos\n",
      "Procesando columna: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 15. Best value: 3.44361: 100%|██████████| 20/20 [06:53<00:00, 20.66s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "`Forecaster` refitted using the best-found lags and parameters, and the whole data set: \n",
      "  Lags: [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48] \n",
      "  Parameters: {'n_estimators': 400, 'max_depth': 9, 'min_data_in_leaf': 319, 'learning_rate': 0.061510068002042555, 'feature_fraction': 0.6, 'max_bin': 200, 'reg_alpha': 0.30000000000000004, 'reg_lambda': 0.2}\n",
      "  Backtesting metric: 3.4436091166867513\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 273/273 [00:08<00:00, 31.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El tiempo empleado para la columna 3 ha sido: 434.94 segundos\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "fin_train = '2023-04-30 23:59:00'\n",
    "fin_val = '2023-09-30 23:59:00'\n",
    "\n",
    "# Listas para almacenar las predicciones, métricas y modelos\n",
    "metricas = []\n",
    "tiempos_procesamiento = []\n",
    "cluster = []\n",
    "\n",
    "predicciones_dict = {}\n",
    "datos_test_dict = {}\n",
    "\n",
    "for columna in df.columns: # SI SE QUIERE EJEMPLO REDUCIDO, REDUCIR LA CANTIDAD DE COLUMNAS\n",
    "    print(f'Procesando columna: {columna}')\n",
    "    inicio_tiempo = time.time()\n",
    "    # todo: LA GENERACIÓN DE VARIABLES EXÓGENAS SE DEBERÍA HACER ANTES, NO VARÍAN CON LA ESTACIÓN\n",
    "    datos = df[columna]\n",
    "    datos = prepare_time_series(datos, column_name='CANTIDAD', freq='H')\n",
    "    datos = imputar_nulos_por_hora(datos)\n",
    "    variables_exogenas = calculo_variables_exogenas(datos)\n",
    "    exog_cols, variables_exogenas = auxiliar(variables_exogenas)\n",
    "    datos, datos_train, datos_val, datos_test = tratamiento(datos, variables_exogenas, fin_train, fin_val)\n",
    "\n",
    "    # Búsqueda de hiperparámetros\n",
    "    # ==============================================================================\n",
    "    forecaster = ForecasterAutoreg(\n",
    "        regressor        = LGBMRegressor(random_state=15926, verbose=-1),\n",
    "        lags             = 48,\n",
    "        transformer_exog = transformer_exog,\n",
    "        fit_kwargs       = {\"categorical_feature\": \"auto\"}\n",
    "    )\n",
    "    \n",
    "    results_search, frozen_trial = bayesian_search_forecaster(\n",
    "        forecaster         = forecaster,\n",
    "        y                  = datos.loc[:fin_val, 'CANTIDAD'],\n",
    "        exog               = datos.loc[:fin_val, exog_cols],\n",
    "        search_space       = search_space,\n",
    "        steps              = 8,\n",
    "        refit              = False,\n",
    "        metric             = 'mean_absolute_error',\n",
    "        initial_train_size = len(datos_train),\n",
    "        fixed_train_size   = False,\n",
    "        n_trials           = 20,\n",
    "        random_state       = 123,\n",
    "        return_best        = True,\n",
    "        n_jobs             = 'auto',\n",
    "        verbose            = False,\n",
    "        show_progress      = True\n",
    "    )\n",
    "\n",
    "    # Backtesting en los datos de test incluyendo las variables exógenas\n",
    "    # ==============================================================================\n",
    "    metrica_LGBMRegressor, predicciones = backtesting_forecaster(\n",
    "        forecaster         = forecaster,\n",
    "        y                  = datos['CANTIDAD'],\n",
    "        exog               = datos[exog_cols],\n",
    "        steps              = 8,\n",
    "        metric             = 'mean_absolute_error',\n",
    "        initial_train_size = len(datos[:fin_val]),\n",
    "        refit              = False,\n",
    "        n_jobs             = 'auto',\n",
    "        verbose            = False,\n",
    "        show_progress      = True\n",
    "    )\n",
    "\n",
    "    # Agregar la métrica a la lista de métricas\n",
    "    metricas.append(metrica_LGBMRegressor['mean_absolute_error'].values[0])\n",
    "\n",
    "    # Guardar predicciones y datos de prueba en diccionarios\n",
    "    predicciones_dict[columna] = predicciones\n",
    "    datos_test_dict[columna] = datos_test['CANTIDAD']\n",
    "    \n",
    "    # Guardar el cluster\n",
    "    cluster.append(info.Cluster[info.station_id == columna].values[0])\n",
    "\n",
    "    # Guardar el forecaster ajustado\n",
    "    with open(f'../../1-DATOS/3-DATOS DE RESULTADOS/PREDICCION/MODELOS/forecaster_{columna}.pkl', 'wb') as f:\n",
    "        cloudpickle.dump(forecaster, f)\n",
    "\n",
    "    fin_tiempo = time.time()\n",
    "    tiempo_empleado = fin_tiempo - inicio_tiempo\n",
    "    tiempos_procesamiento.append(tiempo_empleado)\n",
    "\n",
    "    print(f'El tiempo empleado para la columna {columna} ha sido: {tiempo_empleado:.2f} segundos')\n",
    "    \n",
    "# Convertir los diccionarios a DataFrames finales\n",
    "predicciones_df_final = pd.DataFrame({key: value['pred'] for key, value in predicciones_dict.items()})\n",
    "datos_test_df_final = pd.DataFrame(datos_test_dict)\n",
    "\n",
    "# Crear el DataFrame con dos columnas\n",
    "metricas_df = pd.DataFrame({\n",
    "    'columna': df.columns,\n",
    "    'metrica': metricas,\n",
    "    'tiempo_procesamiento': tiempos_procesamiento,\n",
    "    'cluster': cluster\n",
    "})\n",
    "\n",
    "predicciones_df_final.to_csv('../../1-DATOS/3-DATOS DE RESULTADOS/PREDICCION/predicciones_test.csv')\n",
    "datos_test_df_final.to_csv('../../1-DATOS/3-DATOS DE RESULTADOS/PREDICCION/datos_test.csv')\n",
    "metricas_df.to_csv('../../1-DATOS/3-DATOS DE RESULTADOS/PREDICCION/metricas.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TFM_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
